---
title: "CNP Dataset Cleaning Report"
subtitle: "Credit Card Fraud Detection Dataset - Data Cleaning Pipeline"
author: "Your Name"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    theme: flatly
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 6)
```

# Executive Summary

This report documents the data cleaning process for the Credit Card Fraud Detection (CNP) dataset. The dataset contains anonymized credit card transactions with 284,807 records and 31 features (28 PCA-transformed features, Time, Amount, and Class).

**Dataset Information:**
- **Source**: Kaggle - Machine Learning Group - ULB
- **Original Records**: 284,807
- **Features**: 31 (Time, V1-V28, Amount, Class)
- **Target Variable**: Class (0 = Normal, 1 = Fraud)

---

# Step 1: Data Loading

## 1.1 Dataset Loading

```{r load-data, eval=FALSE}
# Load required libraries
library(readr)
library(dplyr)
library(lubridate)
library(DescTools)

# Load dataset
dataset_path <- "C:/Users/monai/OneDrive - student.uni-halle.de/Desktop/Billie _ R/creditcard.csv"
df <- read_csv(dataset_path, show_col_types = FALSE)
```

**Results:**
- Dataset loaded successfully
- Original dataset shape: `r nrow(df)` rows, `r ncol(df)` columns
- Memory usage: `r format(object.size(df), units = "MB")`

**Column Names:**
- Time, V1-V28 (PCA features), Amount, Class

---

# Step 2: Normalize Identifiers

## 2.1 Column Name Normalization

**Action Taken:**
- Original column names preserved (Time, V1-V28, Amount, Class)
- Transaction ID created for each row

**Results:**
```{r normalize, eval=FALSE}
# Transaction ID created
df <- df %>%
  mutate(transaction_id = row_number(), .before = 1)
```

**Status:** ✓ Identifiers normalized
- Original column names preserved
- Transaction ID added as first column

---

# Step 3: Handle Missing Values

## 3.1 Missing Value Detection

**Action Taken:**
- Checked for missing values across all columns
- Applied appropriate imputation strategies

**Results:**

```
✓ No missing values found in the dataset
✓ Missing values handled. Total remaining: 0
```

**Summary:**
- **Total Missing Values**: 0
- **Columns Checked**: All 31 columns
- **Imputation Strategy**: Not required (no missing values)

**Interpretation:**
The dataset is complete with no missing values, which is excellent for data quality. No imputation was necessary.

---

# Step 4: Convert Timestamps to UTC and Create Derived Fields

## 4.1 Timestamp Conversion

**Action Taken:**
- Converted Time column (seconds) to UTC timestamps
- Created base timestamp: September 1, 2013 00:00:00 UTC
- Derived time-based features

**Results:**

*[Add your results here when you run this step]*

**Derived Fields Created:**
- `transaction_timestamp_utc`: Full UTC timestamp
- `hour_of_day`: Hour (0-23)
- `day_of_week`: Day of week (Mon-Sun)
- `day_of_month`: Day of month (1-31)
- `month`: Month name
- `is_weekend`: Boolean (TRUE/FALSE)
- `time_of_day`: Categorical (Morning/Afternoon/Evening/Night)
- `time_since_previous`: Seconds since previous transaction

**Status:** *[Add status when step is complete]*

---

# Step 5: Winsorize Extreme Amounts to Reduce Skew

## 5.1 Amount Distribution Analysis

**Action Taken:**
- Analyzed original Amount distribution
- Calculated skewness
- Applied winsorization at 1st and 99th percentiles

**Results:**

*[Add your results here when you run this step]*

**Original Statistics:**
- Mean Amount: *[Add value]*
- Median Amount: *[Add value]*
- Min Amount: *[Add value]*
- Max Amount: *[Add value]*
- Skewness: *[Add value]*

**Winsorization:**
- Percentiles: 1st and 99th
- Lower Bound: *[Add value]*
- Upper Bound: *[Add value]*
- Values Capped: *[Add count]*

**Winsorized Statistics:**
- Mean Amount: *[Add value]*
- Median Amount: *[Add value]*
- Skewness: *[Add value]*
- Skewness Reduction: *[Add value]*

**Status:** *[Add status when step is complete]*

---

# Step 6: Additional Data Quality Checks

## 6.1 Data Quality Metrics

**Checks Performed:**
- Duplicate transactions
- Data types verification
- Fraud label distribution

**Results:**

*[Add your results here when you run this step]*

**Duplicate Transactions:** *[Add count]*

**Data Types:**
*[Add data type summary]*

**Fraud Distribution:**
- Normal (Class = 0): *[Add count]* (*[Add percentage]*%)
- Fraud (Class = 1): *[Add count]* (*[Add percentage]*%)

---

# Step 7: Final Dataset Summary

## 7.1 Cleaned Dataset Information

**Final Dataset:**
- **Rows**: *[Add count]*
- **Columns**: *[Add count]*
- **Output Location**: `cnp_dataset/cleaned/creditcard_cleaned.csv`

**Columns in Final Dataset:**
1. transaction_id
2. Time
3. V1-V28 (PCA features)
4. Amount (winsorized)
5. Amount_original
6. Class
7. transaction_timestamp_utc
8. hour_of_day
9. day_of_week
10. day_of_month
11. month
12. is_weekend
13. time_of_day
14. time_since_previous
15. seconds_since_first

---

# Conclusions

## Summary of Cleaning Steps

1. ✓ **Identifiers Normalized**: Transaction ID created, original names preserved
2. ✓ **Missing Values Handled**: No missing values found (0 total)
3. ⏳ **Timestamps Converted**: *[Status]*
4. ⏳ **Amounts Winsorized**: *[Status]*
5. ⏳ **Quality Checks**: *[Status]*

## Data Quality Assessment

**Strengths:**
- No missing values
- Complete dataset
- Standard column format

**Improvements Made:**
- Transaction IDs added
- Timestamps converted to UTC
- Time-based features derived
- Extreme amounts winsorized

**Dataset Ready For:**
- Exploratory Data Analysis (EDA)
- Feature Engineering
- Machine Learning Modeling
- Statistical Analysis

---

# Appendix

## A. Code Repository

All cleaning scripts are available in:
- `clean_cnp_dataset.R`: Main cleaning script
- `load_dataset.R`: Dataset loading script

## B. Package Versions

```{r packages, eval=FALSE}
sessionInfo()
```

## C. References

- Dataset Source: https://www.kaggle.com/mlg-ulb/creditcardfraud/
- License: Open Data Commons Database License (DbCL) v1.0

---

*Report generated on `r Sys.Date()`*






